---
title: "Useful Metrics"
date: 2024-11-16T22:49:30-04:00
draft: false
tags: ["system design", "observability", "errors"]
---
A couple of years ago, I was helping with an acquisition at my company. The company we were acquiring had one product, a B2B web app. One of the things that came up during the first couple of days of reviewing their system setup was their monitoring. They had some very nice looking Grafana dashboards set up. I want to discuss one of them in particular, because I don't really remember what the others said. It almost doesn't matter at this point. The title was something like "Number of Errors", and the developers told us, pretty much verbatim, "As long as the number of errors stays around two hundred thousand, everything is fine". I didn't say this at the time, but what I was thinking was "I'm sorry, 200,000?? That's just a constant amount of errors that are happening at all times??" I should have said it out loud. Then we might have had a more productive discussion about what the variance of that number is. Is 300k "around"? Is 100k "around"? But I was not experienced enough at the time to know that that was a bad way of doing things, even if I may have felt it in my gut, so I just made a note.

The above is an anti-example of what I would call a useful metric. A metric should be giving actionable information. But with a vague threshold like "around 200k", it's very hard to know exactly when to take action. How high does it have to go before I take action?  Ironically, the number of errors going down in this situation might also be an indication that something is wrong, because maybe something that's supposed to be running and producing errors isn't for some reason, and that's a good thing to know. Even something like "when the number of errors goes above 200k" is not great. What if it's at 199k for three weeks in a row? Is that ok? Maybe it is, but my intuition tells me that's not the case if these are in fact "errors". What are we really measuring here? If we are trying to determine the success rate of a process, then that's a different story. For instance, if 200k errors out of 2 million attempted operations is acceptable, then what is much more useful to know is that 10% is the threshold for errors. As long as the percentage of failures stays below 10%, that's ok. But in order to make that more useful, I also need to know what the total number of operations is. Because if something fails catastrophically and only 20k operations are attempted, and less than 2k fail, then I would assume everything is fine. 

I have since come to learn a term for this: system observability. It makes a lot of sense - you need to know exactly what is happening with a complex system in real time. For instance, if procedure A feeds procedure B and B feeds procedure C, I need to know how A, B, and C all performed. It's not good enough to just say "oh, well the number of errors across all three was pretty small". I mean, you *can*, but that's a terrible way to run a business. And it's certainly a pain as a programmer working on the sytem. Wouldn't it be much better to know if it was something about process C is failing? Then you could drill in to process C and start making improvements. Otherwise, I have to start at the beginning and go through all the steps, which is much slower. So do yourselves and your programmers a favor and monitor each process independently, and ideally, keep your error threshold somewhere below 200,000.